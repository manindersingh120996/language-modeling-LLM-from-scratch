# language-modeling-LLM-from-scratch

## Code Explanation for Bigram Probability based character level language generation Model 
- Explanation is generated using CHATgpt and then checked by me for any mistake

**Importing Libraries:** The code begins by importing necessary libraries such as re (regular expressions), torch (PyTorch), and matplotlib.pyplot for visualization.

**Reading and Filtering the Dataset:** The dataset.txt file is read and split into lines to form a dataset. A regular expression pattern is defined to match English letters and spaces. The dataset is filtered using this pattern to retain only names with English letters.

**Statistics of Dataset:** The code prints the first five names in the filtered dataset and the total number of names.

**Dataset Exploration**: Minimum and maximum name lengths are calculated and printed to provide insights into the dataset's characteristics.

**Creating Unique Characters Set:** The code creates a set unique_characters to store all unique characters found in the dataset.

**Sorting Unique Characters:** The set of unique characters is sorted and printed, along with the total number of unique characters.

**Calculating Bigram Occurrences:** A dictionary b_count is used to calculate the occurrences of bigrams in the dataset. Each name is processed, and bigrams are generated by pairing characters. The dictionary b_count stores the count of each bigram.

**Sorting Bigram Occurrences:** The code sorts the bigram occurrences in descending order, providing insights into which character pairs are most common in the dataset.

**Creating Bigram Matrix:** A two-dimensional array N is initialized with zeros using PyTorch. It has dimensions (len(unique_characters)+2, len(unique_characters)+2). The number of unique characters is increased by two to account for special characters \<S\> (start) and \<E\> (end).

**Creating a Lookup Table:** A lookup table stoi (string to index) is created to map characters to their corresponding indices. Special characters \<S\> and \<E\> are added at the beginning.

**Populating the Bigram Matrix:** The code iterates through the dataset, generating bigrams for each name. For each bigram, the corresponding indices in the N matrix are incremented.

**Inversing the Lookup Table:** A dictionary itos (index to string) is created by inverting the stoi dictionary. It maps indices back to characters.

**Visualizing the Bigram Matrix:** The code uses matplotlib to visualize the bigram matrix N. Characters are displayed along the axes, and the values of the matrix are displayed as text within each cell.

**Smoothing the Bigram Matrix:** To handle cases where there are no characters after <E>, a value of 1 is added to all cells of the matrix N. This is done to avoid issues with negative log likelihood calculations.

**Creating Normalized Probability Matrix:** The matrix P is created as a normalized version of the matrix N. It represents probabilities of transitioning from one character to another.

**Generating Random Text:** The code initializes a random generator and a starting index ix. It then generates 20 sequences of characters based on the bigram model. Characters are selected based on the probabilities stored in the matrix P. The generated text is printed, and sequences end when the <E> character is encountered.

**Evaluating Model Quality:** The code calculates the negative log likelihood as a metric to evaluate the model's quality. This metric indicates how well the model aligns with the actual data. The code prints the log likelihood, negative log likelihood, and the average normalized negative log likelihood.
